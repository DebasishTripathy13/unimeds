# Simple Hindi-to-English Whisper Frontend with 5 minute audio working
# Just upload a file and get results

import whisper
import gradio as gr
import torch
import librosa
import numpy as np
import soundfile as sf
import os

# Load Whisper model (only once)
if 'model' not in globals():
    print("ü§ñ Loading Whisper model...")
    try:
        model = whisper.load_model("base")  # Start with base model for stability
        print("‚úÖ Model loaded successfully!")
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        print("Trying to reload...")
        model = whisper.load_model("base", device="cpu")  # Force CPU if GPU fails
        print("‚úÖ Model loaded on CPU!")

def transcribe_audio(audio_file):
    """
    Simple transcription function
    """
    if audio_file is None:
        return "‚ùå Please upload an audio file"
    
    try:
        # Get the file path
        file_path = audio_file.name if hasattr(audio_file, 'name') else audio_file
        
        print(f"üéØ Processing: {os.path.basename(file_path)}")
        
        # Load and preprocess audio
        audio = whisper.load_audio(file_path)
        audio = whisper.pad_or_trim(audio)
        
        # Create mel spectrogram
        mel = whisper.log_mel_spectrogram(audio).to(model.device)
        
        # Detect language
        _, probs = model.detect_language(mel)
        detected_language = max(probs, key=probs.get)
        confidence = max(probs.values())
        
        # Transcribe and translate to English
        options = whisper.DecodingOptions(
            language="hi",      # Hindi input
            task="translate",   # Translate to English
            fp16=torch.cuda.is_available()
        )
        
        result = whisper.decode(model, mel, options)
        
        # Format results
        translation = result.text.strip()
        
        return f"""
üåê **English Translation:**
{translation}

üìä **Detection Info:**
‚Ä¢ Detected Language: {detected_language}
‚Ä¢ Confidence: {confidence:.1%}
‚Ä¢ Status: ‚úÖ Success
        """
        
    except Exception as e:
        return f"‚ùå **Error:** {str(e)}"

# Create Gradio interface with file upload
def create_simple_interface():
    """Create a simple upload and transcribe interface"""
    
    def process_uploaded_file(audio_file):
        """Process the uploaded audio file"""
        if audio_file is None:
            return "‚ùå Please upload an audio file"
        
        try:
            # audio_file is the path to the uploaded file
            print(f"üéØ Processing uploaded file: {os.path.basename(audio_file)}")
            
            # Method 1: Use Whisper's built-in transcribe function (more reliable)
            result = model.transcribe(
                audio_file,
                language="hi",  # Hindi
                task="translate",  # Translate to English
                fp16=False,  # Disable FP16 to avoid GPU issues
                verbose=False
            )
            
            # Extract results
            translation = result["text"].strip()
            detected_language = result.get("language", "hi")
            
            return f"""üåê ENGLISH TRANSLATION:
"{translation}"

üìä DETECTION INFO:
‚Ä¢ File: {os.path.basename(audio_file)}
‚Ä¢ Detected Language: {detected_language}
‚Ä¢ Status: ‚úÖ Success"""
            
        except Exception as e:
            # Fallback method if the above fails
            try:
                print("Trying alternative processing method...")
                
                # Load audio manually
                audio = whisper.load_audio(audio_file)
                audio = whisper.pad_or_trim(audio)
                
                # Create mel spectrogram
                mel = whisper.log_mel_spectrogram(audio)
                
                # Move to CPU if GPU causes issues
                if torch.cuda.is_available():
                    try:
                        mel = mel.to("cuda")
                        model_device = "cuda"
                    except:
                        mel = mel.to("cpu") 
                        model_device = "cpu"
                else:
                    mel = mel.to("cpu")
                    model_device = "cpu"
                
                # Detect language
                _, probs = model.detect_language(mel)
                detected_language = max(probs, key=probs.get)
                confidence = max(probs.values())
                
                # Transcribe with safer options
                options = whisper.DecodingOptions(
                    language="hi",
                    task="translate",
                    fp16=False,  # Disable FP16
                    temperature=0.0
                )
                
                result = whisper.decode(model, mel, options)
                translation = result.text.strip()
                
                return f"""üåê ENGLISH TRANSLATION:
"{translation}"

üìä DETECTION INFO:
‚Ä¢ File: {os.path.basename(audio_file)}
‚Ä¢ Detected Language: {detected_language}
‚Ä¢ Confidence: {confidence:.1%}
‚Ä¢ Processing: {model_device.upper()}
‚Ä¢ Status: ‚úÖ Success (Fallback method)"""
                
            except Exception as e2:
                return f"""‚ùå ERROR: {str(e)}

üîß TROUBLESHOOTING:
‚Ä¢ Original error: {str(e)}
‚Ä¢ Fallback error: {str(e2)}
‚Ä¢ Try a different audio file format
‚Ä¢ Make sure the file is a valid audio file
‚Ä¢ Audio should be less than 10 minutes for best results"""
    
    # Create the interface
    interface = gr.Interface(
        fn=process_uploaded_file,
        inputs=gr.Audio(
            label="üìÅ Upload Hindi Audio File",
            type="filepath",  # This gives us the file path
            sources=["upload"]  # Only allow file upload, not microphone
        ),
        outputs=gr.Textbox(
            label="üéØ Results",
            lines=10,
            show_copy_button=True,
            placeholder="Upload an audio file to see the English translation here..."
        ),
        title="üéµ Hindi ‚Üí English Speech Translator",
        description="""
        **Simple Steps:**
        1. Click the upload area below
        2. Select your Hindi audio file (.mp3, .wav, .m4a, etc.)
        3. Wait for processing
        4. See the English translation!
        
        **Supported formats:** MP3, WAV, M4A, FLAC, OGG
        """,
        theme=gr.themes.Soft(),
        allow_flagging="never"
    )
    
    return interface

# Simple upload function for Colab
def upload_and_transcribe():
    """Simple upload function for Google Colab"""
    from google.colab import files
    
    print("üìÅ Select your Hindi audio file...")
    uploaded = files.upload()
    
    if not uploaded:
        print("‚ùå No file uploaded")
        return
    
    # Process the first uploaded file
    filename = list(uploaded.keys())[0]
    print(f"\nüéØ Processing: {filename}")
    
    try:
        # Load audio
        audio = whisper.load_audio(filename)
        audio = whisper.pad_or_trim(audio)
        mel = whisper.log_mel_spectrogram(audio).to(model.device)
        
        # Detect language
        _, probs = model.detect_language(mel)
        detected_language = max(probs, key=probs.get)
        confidence = max(probs.values())
        
        # Transcribe
        options = whisper.DecodingOptions(
            language="hi",
            task="translate",
            fp16=torch.cuda.is_available()
        )
        
        result = whisper.decode(model, mel, options)
        
        # Display results
        print("\n" + "="*60)
        print("üéØ TRANSCRIPTION RESULTS")
        print("="*60)
        print(f"üìÅ File: {filename}")
        print(f"üó£Ô∏è  Detected Language: {detected_language}")
        print(f"üìä Confidence: {confidence:.1%}")
        print(f"\nüåê ENGLISH TRANSLATION:")
        print("-" * 40)
        print(f'"{result.text.strip()}"')
        print("="*60)
        
        return result.text.strip()
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

# Direct transcription function
def transcribe_file(file_path):
    """Direct transcription function"""
    try:
        print(f"üéØ Transcribing: {file_path}")
        
        # Load and process
        audio = whisper.load_audio(file_path)
        audio = whisper.pad_or_trim(audio)
        mel = whisper.log_mel_spectrogram(audio).to(model.device)
        
        # Transcribe
        options = whisper.DecodingOptions(
            language="hi",
            task="translate",
            fp16=torch.cuda.is_available()
        )
        
        result = whisper.decode(model, mel, options)
        
        print("‚úÖ Transcription completed!")
        print(f"Result: \"{result.text.strip()}\"")
        
        return result.text.strip()
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

print("üéØ SIMPLE WHISPER FRONTEND READY!")
print("="*50)
print("USAGE OPTIONS:")
print("1. Web Interface (Auto-launched) - Upload files in browser")
print("2. upload_and_transcribe() - Upload in Colab")  
print("3. transcribe_file('path/to/file.mp3') - Direct function")

# Launch the simple interface
print("\nüöÄ Launching web interface...")
print("üì± The interface will open with a public URL you can share!")
interface = create_simple_interface()
interface.launch(
    share=True,      # Creates public URL
    debug=False,     # Clean output
    quiet=True       # Less verbose
)
