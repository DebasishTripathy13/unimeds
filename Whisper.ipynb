{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DebasishTripathy13/unimeds/blob/main/Whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9SFn_Wr2dun"
      },
      "outputs": [],
      "source": [
        "print(\"Installing required packages...\")\n",
        "!pip install --quiet faster-whisper\n",
        "!pip install --quiet gradio\n",
        "!pip install --quiet soundfile\n",
        "!pip install --quiet gtts\n",
        "!pip install --quiet pydub\n",
        "!pip install --humanize\n",
        "!pip install --quiet git+https://github.com/guillaumekln/faster-whisper.git\n",
        "!pip install --quiet transformers datasets sacremoses librosa psutil humanize\n",
        "!pip install --quiet huggingface_hub\n",
        "\n",
        "\n",
        "print(\"Installation complete. Please restart the runtime (Runtime -> Restart runtime) and then run the next cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Update the cache path to target the currently used model\n",
        "cache_path = \"/root/.cache/huggingface/hub/models--vasista22--whisper-hindi-small\"\n",
        "\n",
        "# Check if the path exists before attempting to delete it\n",
        "if os.path.exists(cache_path):\n",
        "    print(f\"Removing corrupted cache at: {cache_path}\")\n",
        "    shutil.rmtree(cache_path)\n",
        "    print(\"Cache removed successfully. Please run your transcription code again.\")\n",
        "else:\n",
        "    print(\"Cache directory not found. No action needed.\")"
      ],
      "metadata": {
        "id": "f-URkCIKhBPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9GgD-4qZ2Kws"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "from google.colab import files\n",
        "import faster_whisper\n",
        "import soundfile as sf\n",
        "import io\n",
        "import torch\n",
        "import psutil  # Import psutil for memory usage\n",
        "import humanize  # pip install humanize to make memory output readable\n",
        "import librosa # To get audio duration\n",
        "\n",
        "# --- 2. Audio File Upload ---\n",
        "print(\"\\nPlease select your Hindi audio MP3 file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Assuming only one file is uploaded, get its name\n",
        "if not uploaded:\n",
        "    print(\"No file uploaded. Please upload an audio file.\")\n",
        "    exit()\n",
        "\n",
        "audio_filename = list(uploaded.keys())[0]\n",
        "print(f\"'{audio_filename}' uploaded successfully.\")\n",
        "\n",
        "# --- 3. Define the Speech-to-Text (STT) Function with Measurements ---\n",
        "\n",
        "def process_audio_with_whisper(audio_path: str):\n",
        "    \"\"\"\n",
        "    Processes an audio file using faster-whisper for STT and measures performance.\n",
        "\n",
        "    Args:\n",
        "        audio_path (str): The path to the audio file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - str: The transcribed text.\n",
        "            - float: Time taken for transcription in seconds.\n",
        "            - dict: A dictionary of performance metrics including time, RAM, and VRAM usage.\n",
        "    \"\"\"\n",
        "    print(f\"\\nStarting transcription and translation for: {audio_path}\")\n",
        "\n",
        "    # Measure initial RAM and VRAM usage\n",
        "    process = psutil.Process(os.getpid())\n",
        "    initial_ram = process.memory_info().rss\n",
        "    initial_vram = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "\n",
        "    model_size = \"large-v2\"\n",
        "    print(f\"Loading Faster-Whisper model ({model_size})...\")\n",
        "    start_model_load_time = time.time()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # To measure VRAM, we need to clear the cache before loading the model\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "    model = faster_whisper.WhisperModel(model_size, device=device, compute_type=\"int8\")\n",
        "    end_model_load_time = time.time()\n",
        "    model_load_time = end_model_load_time - start_model_load_time\n",
        "    print(f\"Model loaded in {model_load_time:.2f} seconds.\")\n",
        "\n",
        "    # Measure RAM and VRAM usage after model loading\n",
        "    ram_after_load = process.memory_info().rss\n",
        "    vram_after_load = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "\n",
        "    # Perform translation\n",
        "    start_transcription_time = time.time()\n",
        "    # It's good practice to get the audio duration for context\n",
        "    try:\n",
        "        audio_duration = librosa.get_duration(filename=audio_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not get audio duration: {e}\")\n",
        "        audio_duration = None\n",
        "\n",
        "    segments, info = model.transcribe(audio_path, beam_size=5, language=\"hi\", task=\"translate\")\n",
        "    end_transcription_time = time.time()\n",
        "    translation_time = end_transcription_time - start_transcription_time\n",
        "\n",
        "    # Measure RAM and VRAM usage after transcription\n",
        "    ram_after_transcribe = process.memory_info().rss\n",
        "    vram_after_transcribe = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
        "\n",
        "    translated_text = \"\"\n",
        "    print(\"\\nTranslation results (English text):\")\n",
        "    for segment in segments:\n",
        "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
        "        translated_text += segment.text + \" \"\n",
        "\n",
        "    # --- Performance Metrics Dictionary ---\n",
        "    metrics = {\n",
        "        \"model_load_time\": model_load_time,\n",
        "        \"translation_time\": translation_time,\n",
        "        \"total_processing_time\": model_load_time + translation_time,\n",
        "        \"initial_ram_usage\": humanize.naturalsize(initial_ram),\n",
        "        \"peak_ram_usage\": humanize.naturalsize(ram_after_transcribe),\n",
        "        \"ram_consumed_by_model\": humanize.naturalsize(ram_after_load - initial_ram),\n",
        "        \"ram_consumed_by_transcription\": humanize.naturalsize(ram_after_transcribe - ram_after_load),\n",
        "    }\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        metrics[\"initial_vram_usage\"] = humanize.naturalsize(initial_vram)\n",
        "        metrics[\"peak_vram_usage\"] = humanize.naturalsize(vram_after_transcribe)\n",
        "        metrics[\"vram_consumed_by_model\"] = humanize.naturalsize(vram_after_load - initial_vram)\n",
        "        metrics[\"vram_consumed_by_transcription\"] = humanize.naturalsize(vram_after_transcribe - vram_after_load)\n",
        "\n",
        "    if audio_duration:\n",
        "        metrics[\"audio_duration\"] = audio_duration\n",
        "        metrics[\"real_time_factor\"] = translation_time / audio_duration\n",
        "\n",
        "    return translated_text.strip(), translation_time, metrics\n",
        "\n",
        "# --- 4. Execute the STT Process and Display Results ---\n",
        "print(\"\\n--- Starting Audio Processing (Hindi to English Translation) ---\")\n",
        "try:\n",
        "    transcribed_text, time_taken, performance_metrics = process_audio_with_whisper(audio_filename)\n",
        "\n",
        "    print(\"\\n--- Summary of Run ---\")\n",
        "    print(f\"**Translated Text (English):**\\n{transcribed_text}\")\n",
        "    print(\"\\n**Performance Metrics:**\")\n",
        "    print(f\"- **Total Translation Time:** {performance_metrics['translation_time']:.2f} seconds\")\n",
        "    print(f\"- **Total Processing Time (including model load):** {performance_metrics['total_processing_time']:.2f} seconds\")\n",
        "\n",
        "    if 'audio_duration' in performance_metrics:\n",
        "        print(f\"- **Audio Duration:** {performance_metrics['audio_duration']:.2f} seconds\")\n",
        "        print(f\"- **Real-Time Factor (RTF):** {performance_metrics['real_time_factor']:.2f} (a value < 1 is faster than real-time)\")\n",
        "\n",
        "    print(\"\\n**Memory Usage:**\")\n",
        "    print(f\"- **Initial RAM Usage:** {performance_metrics['initial_ram_usage']}\")\n",
        "    print(f\"- **Peak RAM Usage:** {performance_metrics['peak_ram_usage']}\")\n",
        "    print(f\"- **RAM Consumed by Model:** {performance_metrics['ram_consumed_by_model']}\")\n",
        "    print(f\"- **RAM Consumed During Transcription:** {performance_metrics['ram_consumed_by_transcription']}\")\n",
        "\n",
        "    if 'peak_vram_usage' in performance_metrics:\n",
        "        print(f\"\\n**VRAM (GPU Memory) Usage:**\")\n",
        "        print(f\"- **Initial VRAM Usage:** {performance_metrics['initial_vram_usage']}\")\n",
        "        print(f\"- **Peak VRAM Usage:** {performance_metrics['peak_vram_usage']}\")\n",
        "        print(f\"- **VRAM Consumed by Model:** {performance_metrics['vram_consumed_by_model']}\")\n",
        "        print(f\"- **VRAM Consumed During Transcription:** {performance_metrics['vram_consumed_by_transcription']}\")\n",
        "\n",
        "    print(\"\\n**Note on Accuracy/Precision (for Translation):** To measure translation accuracy (e.g., BLEU score), you need to provide a 'ground truth' (a manually verified correct English translation) of your audio. Without it, these metrics cannot be calculated.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please ensure your audio file is valid, necessary packages are installed, and your Colab runtime is set to GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "from google.colab import files\n",
        "import faster_whisper\n",
        "import soundfile as sf\n",
        "import io\n",
        "import torch\n",
        "import psutil\n",
        "import humanize\n",
        "import librosa\n",
        "import getpass\n",
        "\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "\n",
        "# ==============================\n",
        "# 1. Hugging Face Authentication (Optional)\n",
        "# ==============================\n",
        "def setup_huggingface_login():\n",
        "    print(\"=== Hugging Face Authentication ===\")\n",
        "    print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
        "\n",
        "    use_token = input(\"\\nDo you want to login with a Hugging Face token? (y/n): \").lower().strip()\n",
        "    global hf_token\n",
        "    hf_token = None\n",
        "\n",
        "    if use_token in ['y', 'yes']:\n",
        "        token = getpass.getpass(\"Enter your Hugging Face access token: \")\n",
        "        if token.strip():\n",
        "            try:\n",
        "                login(token=token.strip())\n",
        "                hf_token = token.strip()\n",
        "                print(\"âœ… Successfully logged in to Hugging Face!\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Login failed: {str(e)}\")\n",
        "                print(\"Continuing without authentication...\")\n",
        "                return False\n",
        "        else:\n",
        "            print(\"Empty token provided. Continuing without authentication...\")\n",
        "            return False\n",
        "    else:\n",
        "        print(\"Continuing without Hugging Face authentication...\")\n",
        "        return False\n",
        "\n",
        "hf_authenticated = setup_huggingface_login()\n",
        "\n",
        "# ==============================\n",
        "# 2. Audio Upload\n",
        "# ==============================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"AUDIO FILE UPLOAD\")\n",
        "print(\"=\"*50)\n",
        "print(\"Please select your Hindi audio MP3 file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"âŒ No file uploaded. Please upload an audio file.\")\n",
        "    exit()\n",
        "\n",
        "audio_filename = list(uploaded.keys())[0]\n",
        "print(f\"âœ… '{audio_filename}' uploaded successfully.\")\n",
        "\n",
        "# ==============================\n",
        "# 3. Transcription with Collabora Hindi Model\n",
        "# ==============================\n",
        "def process_audio_with_collabora(audio_path: str):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"TRANSCRIPTION PROCESS\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"ðŸŽµ Processing audio file: {audio_path}\")\n",
        "    print(\"ðŸ¤– Using model: collabora/faster-whisper-small-hindi\")\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    initial_ram = process.memory_info().rss\n",
        "    print(f\"\\nðŸ“Š Initial RAM: {humanize.naturalsize(initial_ram)}\")\n",
        "\n",
        "    device = \"cpu\"\n",
        "    print(f\"   Device: {device}\")\n",
        "\n",
        "    start_model_load_time = time.time()\n",
        "    try:\n",
        "        # Load Collabora's Hindi Whisper model (SMALL)\n",
        "        model = faster_whisper.WhisperModel(\"collabora/faster-whisper-small-hindi\",\n",
        "                                            device=device,\n",
        "                                            compute_type=\"int8\")\n",
        "        model_load_time = time.time() - start_model_load_time\n",
        "        print(f\"âœ… Model loaded in {model_load_time:.2f} seconds.\")\n",
        "\n",
        "        start_transcription_time = time.time()\n",
        "        audio_duration = librosa.get_duration(filename=audio_path)\n",
        "        print(f\"\\nðŸŽµ Audio duration: {audio_duration:.2f} seconds\")\n",
        "\n",
        "        segments, info = model.transcribe(\n",
        "            audio_path,\n",
        "            language=\"hi\",\n",
        "            beam_size=5,\n",
        "            vad_filter=True,\n",
        "            vad_parameters=dict(min_silence_duration_ms=500)\n",
        "        )\n",
        "\n",
        "        transcribed_text = \" \".join([segment.text for segment in segments])\n",
        "        transcription_time = time.time() - start_transcription_time\n",
        "\n",
        "        metrics = {\n",
        "            \"model_name\": \"collabora/faster-whisper-small-hindi\",\n",
        "            \"device\": device,\n",
        "            \"model_load_time\": model_load_time,\n",
        "            \"transcription_time\": transcription_time,\n",
        "            \"total_time\": model_load_time + transcription_time,\n",
        "            \"segments_processed\": len(transcribed_text.split())\n",
        "        }\n",
        "\n",
        "        return transcribed_text.strip(), transcription_time, metrics, info\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during transcription: {str(e)}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "result = process_audio_with_collabora(audio_filename)\n",
        "\n",
        "# ==============================\n",
        "# 4. Translation (Always Run)\n",
        "# ==============================\n",
        "def translate_hindi_to_english(text):\n",
        "    print(\"\\nðŸŒ Translating via Hugging Face Transformers...\")\n",
        "    translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-hi-en\", device=-1)\n",
        "    translated = translator(text, max_length=512)\n",
        "    return translated[0]['translation_text']\n",
        "\n",
        "# ==============================\n",
        "# 5. Save and Download Results\n",
        "# ==============================\n",
        "if result[0] is not None:\n",
        "    transcribed_text, transcription_time, metrics, info = result\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"TRANSCRIBED TEXT (Hindi)\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(transcribed_text)\n",
        "\n",
        "    english_translation = translate_hindi_to_english(transcribed_text)\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"TRANSLATED TEXT (English)\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(english_translation)\n",
        "\n",
        "    output_filename = f\"transcription_translation_smallhindi_{int(time.time())}.txt\"\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"Model: collabora/faster-whisper-small-hindi\\n\")\n",
        "        f.write(f\"Audio File: {audio_filename}\\n\")\n",
        "        f.write(f\"Transcription Time: {transcription_time:.2f} seconds\\n\")\n",
        "        if info:\n",
        "            f.write(f\"Detected Language: {info.language}\\n\")\n",
        "            f.write(f\"Language Probability: {info.language_probability:.2f}\\n\")\n",
        "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "        f.write(\"TRANSCRIBED TEXT (Hindi)\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\")\n",
        "        f.write(transcribed_text + \"\\n\")\n",
        "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "        f.write(\"TRANSLATED TEXT (English)\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\")\n",
        "        f.write(english_translation + \"\\n\")\n",
        "\n",
        "    print(f\"\\nðŸ’¾ Saved to: {output_filename}\")\n",
        "    try:\n",
        "        files.download(output_filename)\n",
        "    except:\n",
        "        pass\n",
        "else:\n",
        "    print(\"âŒ Transcription failed. Please check your audio file and try again.\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"PROCESS COMPLETED\")\n",
        "print(f\"{'='*50}\")\n"
      ],
      "metadata": {
        "id": "eCtnyGbVd8KU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP3SCYpw6q9AxqhzTtwgHBs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}